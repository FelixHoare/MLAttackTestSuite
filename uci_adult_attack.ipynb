{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A notebook to implement subpopulation attacks on the UCI Adult dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import neural_network, linear_model\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (32561, 15)\n",
      "Test shape: (16282, 15)\n",
      "UCI dataset shape: (48843, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>captial-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646.0</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721.0</td>\n",
       "      <td>11th</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age          workclass    fnlwgt   education  education-num  \\\n",
       "0  39          State-gov   77516.0   Bachelors           13.0   \n",
       "1  50   Self-emp-not-inc   83311.0   Bachelors           13.0   \n",
       "2  38            Private  215646.0     HS-grad            9.0   \n",
       "3  53            Private  234721.0        11th            7.0   \n",
       "4  28            Private  338409.0   Bachelors           13.0   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   captial-gain  capital-loss  hours-per-week  native-country  income  \n",
       "0        2174.0           0.0            40.0   United-States   <=50K  \n",
       "1           0.0           0.0            13.0   United-States   <=50K  \n",
       "2           0.0           0.0            40.0   United-States   <=50K  \n",
       "3           0.0           0.0            40.0   United-States   <=50K  \n",
       "4           0.0           0.0            40.0            Cuba   <=50K  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import UCI Adult datasets as pandas dataframes\n",
    "\n",
    "uci_train_data = pd.read_csv('data/adult/adult.data', header=None, names = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'captial-gain',\n",
    "    'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
    "\n",
    "print(f'Training shape: {uci_train_data.shape}')\n",
    "\n",
    "uci_test_data = pd.read_csv('data/adult/adult.test', header=None, names = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'captial-gain',\n",
    "    'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
    "\n",
    "print(f'Test shape: {uci_test_data.shape}')\n",
    "\n",
    "uci_data = pd.concat([uci_train_data, uci_test_data], axis=0)\n",
    "print(f'UCI dataset shape: {uci_data.shape}')\n",
    "uci_data.head()\n",
    "\n",
    "# Initially 15 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCI dataset shape after dropping columns: (48843, 12)\n",
      "UCI dataset shape after one-hot encoding: (48843, 60)\n",
      "['age', 'captial-gain', 'capital-loss', 'hours-per-week', 'workclass_ Federal-gov', 'workclass_ Local-gov', 'workclass_ Never-worked', 'workclass_ Private', 'workclass_ Self-emp-inc', 'workclass_ Self-emp-not-inc', 'workclass_ State-gov', 'workclass_ Without-pay', 'education_2.0', 'education_3.0', 'education_4.0', 'education_5.0', 'education_6.0', 'education_7.0', 'education_8.0', 'education_9.0', 'education_10.0', 'education_11.0', 'education_12.0', 'education_13.0', 'education_14.0', 'education_15.0', 'education_16.0', 'marital_ Married-AF-spouse', 'marital_ Married-civ-spouse', 'marital_ Married-spouse-absent', 'marital_ Never-married', 'marital_ Separated', 'marital_ Widowed', 'occupation_ Adm-clerical', 'occupation_ Armed-Forces', 'occupation_ Craft-repair', 'occupation_ Exec-managerial', 'occupation_ Farming-fishing', 'occupation_ Handlers-cleaners', 'occupation_ Machine-op-inspct', 'occupation_ Other-service', 'occupation_ Priv-house-serv', 'occupation_ Prof-specialty', 'occupation_ Protective-serv', 'occupation_ Sales', 'occupation_ Tech-support', 'occupation_ Transport-moving', 'relationship_ Not-in-family', 'relationship_ Other-relative', 'relationship_ Own-child', 'relationship_ Unmarried', 'relationship_ Wife', 'race_ Asian-Pac-Islander', 'race_ Black', 'race_ Other', 'race_ White', 'sex_ Male', 'income_ <=50K.', 'income_ >50K', 'income_ >50K.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>captial-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_ Federal-gov</th>\n",
       "      <th>workclass_ Local-gov</th>\n",
       "      <th>workclass_ Never-worked</th>\n",
       "      <th>workclass_ Private</th>\n",
       "      <th>workclass_ Self-emp-inc</th>\n",
       "      <th>workclass_ Self-emp-not-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>relationship_ Unmarried</th>\n",
       "      <th>relationship_ Wife</th>\n",
       "      <th>race_ Asian-Pac-Islander</th>\n",
       "      <th>race_ Black</th>\n",
       "      <th>race_ Other</th>\n",
       "      <th>race_ White</th>\n",
       "      <th>sex_ Male</th>\n",
       "      <th>income_ &lt;=50K.</th>\n",
       "      <th>income_ &gt;50K</th>\n",
       "      <th>income_ &gt;50K.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  age  captial-gain  capital-loss  hours-per-week  workclass_ Federal-gov  \\\n",
       "0  39        2174.0           0.0            40.0                       0   \n",
       "1  50           0.0           0.0            13.0                       0   \n",
       "2  38           0.0           0.0            40.0                       0   \n",
       "3  53           0.0           0.0            40.0                       0   \n",
       "4  28           0.0           0.0            40.0                       0   \n",
       "\n",
       "   workclass_ Local-gov  workclass_ Never-worked  workclass_ Private  \\\n",
       "0                     0                        0                   0   \n",
       "1                     0                        0                   0   \n",
       "2                     0                        0                   1   \n",
       "3                     0                        0                   1   \n",
       "4                     0                        0                   1   \n",
       "\n",
       "   workclass_ Self-emp-inc  workclass_ Self-emp-not-inc  ...  \\\n",
       "0                        0                            0  ...   \n",
       "1                        0                            1  ...   \n",
       "2                        0                            0  ...   \n",
       "3                        0                            0  ...   \n",
       "4                        0                            0  ...   \n",
       "\n",
       "   relationship_ Unmarried  relationship_ Wife  race_ Asian-Pac-Islander  \\\n",
       "0                        0                   0                         0   \n",
       "1                        0                   0                         0   \n",
       "2                        0                   0                         0   \n",
       "3                        0                   0                         0   \n",
       "4                        0                   1                         0   \n",
       "\n",
       "   race_ Black  race_ Other  race_ White  sex_ Male  income_ <=50K.  \\\n",
       "0            0            0            1          1               0   \n",
       "1            0            0            1          1               0   \n",
       "2            0            0            1          1               0   \n",
       "3            1            0            0          1               0   \n",
       "4            1            0            0          0               0   \n",
       "\n",
       "   income_ >50K  income_ >50K.  \n",
       "0             0              0  \n",
       "1             0              0  \n",
       "2             0              0  \n",
       "3             0              0  \n",
       "4             0              0  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paper drops the following columns, due to siginificant correlation with other columns:\n",
    "# education\n",
    "# native-country\n",
    "# fnlwgt\n",
    "\n",
    "uci_data = uci_data.drop(columns=['education', 'native-country', 'fnlwgt'], axis=1)\n",
    "print(f'UCI dataset shape after dropping columns: {uci_data.shape}')\n",
    "\n",
    "# Paper then one-hot encodes all columns that are categorical values (plus education-num):\n",
    "# workclass\n",
    "# education\n",
    "# marital-status\n",
    "# occupation\n",
    "# relationship\n",
    "# race\n",
    "\n",
    "categorical_columns = ['workclass', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'income']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    if '-' in column:\n",
    "        column_prefix = column.split('-')[0]\n",
    "    else:\n",
    "        column_prefix = column\n",
    "\n",
    "    uci_data = pd.concat([uci_data, pd.get_dummies(uci_data[column], prefix=column_prefix, drop_first=True)], axis=1)\n",
    "    uci_data = uci_data.drop(column, axis=1)\n",
    "\n",
    "print(f'UCI dataset shape after one-hot encoding: {uci_data.shape}')\n",
    "print(list(uci_data.columns))\n",
    "uci_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                               0\n",
       "captial-gain                      1\n",
       "capital-loss                      1\n",
       "hours-per-week                    1\n",
       "workclass_ Federal-gov            0\n",
       "workclass_ Local-gov              0\n",
       "workclass_ Never-worked           0\n",
       "workclass_ Private                0\n",
       "workclass_ Self-emp-inc           0\n",
       "workclass_ Self-emp-not-inc       0\n",
       "workclass_ State-gov              0\n",
       "workclass_ Without-pay            0\n",
       "education_2.0                     0\n",
       "education_3.0                     0\n",
       "education_4.0                     0\n",
       "education_5.0                     0\n",
       "education_6.0                     0\n",
       "education_7.0                     0\n",
       "education_8.0                     0\n",
       "education_9.0                     0\n",
       "education_10.0                    0\n",
       "education_11.0                    0\n",
       "education_12.0                    0\n",
       "education_13.0                    0\n",
       "education_14.0                    0\n",
       "education_15.0                    0\n",
       "education_16.0                    0\n",
       "marital_ Married-AF-spouse        0\n",
       "marital_ Married-civ-spouse       0\n",
       "marital_ Married-spouse-absent    0\n",
       "marital_ Never-married            0\n",
       "marital_ Separated                0\n",
       "marital_ Widowed                  0\n",
       "occupation_ Adm-clerical          0\n",
       "occupation_ Armed-Forces          0\n",
       "occupation_ Craft-repair          0\n",
       "occupation_ Exec-managerial       0\n",
       "occupation_ Farming-fishing       0\n",
       "occupation_ Handlers-cleaners     0\n",
       "occupation_ Machine-op-inspct     0\n",
       "occupation_ Other-service         0\n",
       "occupation_ Priv-house-serv       0\n",
       "occupation_ Prof-specialty        0\n",
       "occupation_ Protective-serv       0\n",
       "occupation_ Sales                 0\n",
       "occupation_ Tech-support          0\n",
       "occupation_ Transport-moving      0\n",
       "relationship_ Not-in-family       0\n",
       "relationship_ Other-relative      0\n",
       "relationship_ Own-child           0\n",
       "relationship_ Unmarried           0\n",
       "relationship_ Wife                0\n",
       "race_ Asian-Pac-Islander          0\n",
       "race_ Black                       0\n",
       "race_ Other                       0\n",
       "race_ White                       0\n",
       "sex_ Male                         0\n",
       "income_ <=50K.                    0\n",
       "income_ >50K                      0\n",
       "income_ >50K.                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find any columns will null/NaN values\n",
    "uci_data.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCI dataset before dropping rows containing NAN values: (48843, 60)\n",
      "UCI dataset after dropping rows containing NAN values: (48842, 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z9/cm7sm3g127n68j1yhvddfqjc0000gn/T/ipykernel_2735/2205887389.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.dropna will be keyword-only.\n",
      "  uci_data = uci_data.dropna(0)\n"
     ]
    }
   ],
   "source": [
    "# Drop any rows containing NaN values\n",
    "print(f'UCI dataset before dropping rows containing NAN values: {uci_data.shape}')\n",
    "uci_data = uci_data.dropna(0)\n",
    "print(f'UCI dataset after dropping rows containing NAN values: {uci_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                               0\n",
       "captial-gain                      0\n",
       "capital-loss                      0\n",
       "hours-per-week                    0\n",
       "workclass_ Federal-gov            0\n",
       "workclass_ Local-gov              0\n",
       "workclass_ Never-worked           0\n",
       "workclass_ Private                0\n",
       "workclass_ Self-emp-inc           0\n",
       "workclass_ Self-emp-not-inc       0\n",
       "workclass_ State-gov              0\n",
       "workclass_ Without-pay            0\n",
       "education_2.0                     0\n",
       "education_3.0                     0\n",
       "education_4.0                     0\n",
       "education_5.0                     0\n",
       "education_6.0                     0\n",
       "education_7.0                     0\n",
       "education_8.0                     0\n",
       "education_9.0                     0\n",
       "education_10.0                    0\n",
       "education_11.0                    0\n",
       "education_12.0                    0\n",
       "education_13.0                    0\n",
       "education_14.0                    0\n",
       "education_15.0                    0\n",
       "education_16.0                    0\n",
       "marital_ Married-AF-spouse        0\n",
       "marital_ Married-civ-spouse       0\n",
       "marital_ Married-spouse-absent    0\n",
       "marital_ Never-married            0\n",
       "marital_ Separated                0\n",
       "marital_ Widowed                  0\n",
       "occupation_ Adm-clerical          0\n",
       "occupation_ Armed-Forces          0\n",
       "occupation_ Craft-repair          0\n",
       "occupation_ Exec-managerial       0\n",
       "occupation_ Farming-fishing       0\n",
       "occupation_ Handlers-cleaners     0\n",
       "occupation_ Machine-op-inspct     0\n",
       "occupation_ Other-service         0\n",
       "occupation_ Priv-house-serv       0\n",
       "occupation_ Prof-specialty        0\n",
       "occupation_ Protective-serv       0\n",
       "occupation_ Sales                 0\n",
       "occupation_ Tech-support          0\n",
       "occupation_ Transport-moving      0\n",
       "relationship_ Not-in-family       0\n",
       "relationship_ Other-relative      0\n",
       "relationship_ Own-child           0\n",
       "relationship_ Unmarried           0\n",
       "relationship_ Wife                0\n",
       "race_ Asian-Pac-Islander          0\n",
       "race_ Black                       0\n",
       "race_ Other                       0\n",
       "race_ White                       0\n",
       "sex_ Male                         0\n",
       "income_ <=50K.                    0\n",
       "income_ >50K                      0\n",
       "income_ >50K.                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check that all null/NaN values have been removed\n",
    "uci_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCI dataset before grouping income columns: (48842, 60)\n",
      "UCI dataset after grouping income columns: (48842, 59)\n",
      "All UCI columns names:\n",
      "['age', 'captial-gain', 'capital-loss', 'hours-per-week', 'workclass_ Federal-gov', 'workclass_ Local-gov', 'workclass_ Never-worked', 'workclass_ Private', 'workclass_ Self-emp-inc', 'workclass_ Self-emp-not-inc', 'workclass_ State-gov', 'workclass_ Without-pay', 'education_2.0', 'education_3.0', 'education_4.0', 'education_5.0', 'education_6.0', 'education_7.0', 'education_8.0', 'education_9.0', 'education_10.0', 'education_11.0', 'education_12.0', 'education_13.0', 'education_14.0', 'education_15.0', 'education_16.0', 'marital_ Married-AF-spouse', 'marital_ Married-civ-spouse', 'marital_ Married-spouse-absent', 'marital_ Never-married', 'marital_ Separated', 'marital_ Widowed', 'occupation_ Adm-clerical', 'occupation_ Armed-Forces', 'occupation_ Craft-repair', 'occupation_ Exec-managerial', 'occupation_ Farming-fishing', 'occupation_ Handlers-cleaners', 'occupation_ Machine-op-inspct', 'occupation_ Other-service', 'occupation_ Priv-house-serv', 'occupation_ Prof-specialty', 'occupation_ Protective-serv', 'occupation_ Sales', 'occupation_ Tech-support', 'occupation_ Transport-moving', 'relationship_ Not-in-family', 'relationship_ Other-relative', 'relationship_ Own-child', 'relationship_ Unmarried', 'relationship_ Wife', 'race_ Asian-Pac-Islander', 'race_ Black', 'race_ Other', 'race_ White', 'sex_ Male', 'income_ <=50K.', 'income_ >50K.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>captial-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_ Federal-gov</th>\n",
       "      <th>workclass_ Local-gov</th>\n",
       "      <th>workclass_ Never-worked</th>\n",
       "      <th>workclass_ Private</th>\n",
       "      <th>workclass_ Self-emp-inc</th>\n",
       "      <th>workclass_ Self-emp-not-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>relationship_ Own-child</th>\n",
       "      <th>relationship_ Unmarried</th>\n",
       "      <th>relationship_ Wife</th>\n",
       "      <th>race_ Asian-Pac-Islander</th>\n",
       "      <th>race_ Black</th>\n",
       "      <th>race_ Other</th>\n",
       "      <th>race_ White</th>\n",
       "      <th>sex_ Male</th>\n",
       "      <th>income_ &lt;=50K.</th>\n",
       "      <th>income_ &gt;50K.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  age  captial-gain  capital-loss  hours-per-week  workclass_ Federal-gov  \\\n",
       "0  39        2174.0           0.0            40.0                       0   \n",
       "1  50           0.0           0.0            13.0                       0   \n",
       "2  38           0.0           0.0            40.0                       0   \n",
       "3  53           0.0           0.0            40.0                       0   \n",
       "4  28           0.0           0.0            40.0                       0   \n",
       "\n",
       "   workclass_ Local-gov  workclass_ Never-worked  workclass_ Private  \\\n",
       "0                     0                        0                   0   \n",
       "1                     0                        0                   0   \n",
       "2                     0                        0                   1   \n",
       "3                     0                        0                   1   \n",
       "4                     0                        0                   1   \n",
       "\n",
       "   workclass_ Self-emp-inc  workclass_ Self-emp-not-inc  ...  \\\n",
       "0                        0                            0  ...   \n",
       "1                        0                            1  ...   \n",
       "2                        0                            0  ...   \n",
       "3                        0                            0  ...   \n",
       "4                        0                            0  ...   \n",
       "\n",
       "   relationship_ Own-child  relationship_ Unmarried  relationship_ Wife  \\\n",
       "0                        0                        0                   0   \n",
       "1                        0                        0                   0   \n",
       "2                        0                        0                   0   \n",
       "3                        0                        0                   0   \n",
       "4                        0                        0                   1   \n",
       "\n",
       "   race_ Asian-Pac-Islander  race_ Black  race_ Other  race_ White  sex_ Male  \\\n",
       "0                         0            0            0            1          1   \n",
       "1                         0            0            0            1          1   \n",
       "2                         0            0            0            1          1   \n",
       "3                         0            1            0            0          1   \n",
       "4                         0            1            0            0          0   \n",
       "\n",
       "   income_ <=50K.  income_ >50K.  \n",
       "0               0              0  \n",
       "1               0              0  \n",
       "2               0              0  \n",
       "3               0              0  \n",
       "4               0              0  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the two \"income_ >50K\" columns into one\n",
    "print(f'UCI dataset before grouping income columns: {uci_data.shape}')\n",
    "uci_data['income_ >50K.'] = uci_data.pop('income_ >50K.') + uci_data.pop('income_ >50K')\n",
    "print(f'UCI dataset after grouping income columns: {uci_data.shape}')\n",
    "print(\"All UCI columns names:\")\n",
    "print(list(uci_data.columns))\n",
    "uci_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: age\n",
      "1: captial-gain\n",
      "2: capital-loss\n",
      "3: hours-per-week\n",
      "4: workclass_ Federal-gov\n",
      "5: workclass_ Local-gov\n",
      "6: workclass_ Never-worked\n",
      "7: workclass_ Private\n",
      "8: workclass_ Self-emp-inc\n",
      "9: workclass_ Self-emp-not-inc\n",
      "10: workclass_ State-gov\n",
      "11: workclass_ Without-pay\n",
      "12: education_2.0\n",
      "13: education_3.0\n",
      "14: education_4.0\n",
      "15: education_5.0\n",
      "16: education_6.0\n",
      "17: education_7.0\n",
      "18: education_8.0\n",
      "19: education_9.0\n",
      "20: education_10.0\n",
      "21: education_11.0\n",
      "22: education_12.0\n",
      "23: education_13.0\n",
      "24: education_14.0\n",
      "25: education_15.0\n",
      "26: education_16.0\n",
      "27: marital_ Married-AF-spouse\n",
      "28: marital_ Married-civ-spouse\n",
      "29: marital_ Married-spouse-absent\n",
      "30: marital_ Never-married\n",
      "31: marital_ Separated\n",
      "32: marital_ Widowed\n",
      "33: occupation_ Adm-clerical\n",
      "34: occupation_ Armed-Forces\n",
      "35: occupation_ Craft-repair\n",
      "36: occupation_ Exec-managerial\n",
      "37: occupation_ Farming-fishing\n",
      "38: occupation_ Handlers-cleaners\n",
      "39: occupation_ Machine-op-inspct\n",
      "40: occupation_ Other-service\n",
      "41: occupation_ Priv-house-serv\n",
      "42: occupation_ Prof-specialty\n",
      "43: occupation_ Protective-serv\n",
      "44: occupation_ Sales\n",
      "45: occupation_ Tech-support\n",
      "46: occupation_ Transport-moving\n",
      "47: relationship_ Not-in-family\n",
      "48: relationship_ Other-relative\n",
      "49: relationship_ Own-child\n",
      "50: relationship_ Unmarried\n",
      "51: relationship_ Wife\n",
      "52: race_ Asian-Pac-Islander\n",
      "53: race_ Black\n",
      "54: race_ Other\n",
      "55: race_ White\n",
      "56: sex_ Male\n",
      "57: income_ <=50K.\n",
      "58: income_ >50K.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(uci_data.columns)):\n",
    "    print(f'{i}: {uci_data.columns[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First entry in numpy data array: [39 2174.0 0.0 40.0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0]\n",
      "Shape of entry: (59,)\n",
      "Shape of label data: (48842,)\n",
      "First 10 entries of label data: [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "Shape of clean x data without label data: (48842, 57)\n",
      "First entry of x data: [3.900e+01 2.174e+03 0.000e+00 4.000e+01 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      " 1.000e+00]\n",
      "Shape of x_train: (32561, 57), Shape of y_train: (32561,)\n",
      "Shape of x_test: (16281, 57), Shape of y_test: (16281,)\n",
      "Shape of training data: X: (16280, 57), Y: (16280,)\n",
      "Shape of auxiliary data: X: (16281, 57), Y: (16281,)\n",
      "Shape of test data: X: (16281, 57), Y: (16281,)\n"
     ]
    }
   ],
   "source": [
    "# Convert the data to numpy array so that it can be used properly in the models\n",
    "uci_data_np = uci_data.to_numpy()\n",
    "\n",
    "print(f'First entry in numpy data array: {uci_data_np[0]}')\n",
    "print(f'Shape of entry: {uci_data_np[0].shape}')\n",
    "\n",
    "y = (uci_data_np[:, -1]).astype(np.float32) # income > 50K saved as y\n",
    "# This is the target data. Will be training the model to predict if an individual has an income over 50K based on the other features\n",
    "\n",
    "print(f'Shape of label data: {y.shape}')\n",
    "print(f'First 10 entries of label data: {y[:10]}')\n",
    "\n",
    "x = np.delete(uci_data_np, [uci_data_np.shape[1]-1, uci_data_np.shape[1]-2], axis=1) # x data is all other columns, minus the last 2 - income below and above 50K\n",
    "x = x.astype(np.float32)\n",
    "# this is so that no income data is included in the x data\n",
    "\n",
    "print(f'Shape of clean x data without label data: {x.shape}')\n",
    "print(f'First entry of x data: {x[0]}')\n",
    "\n",
    "# Split the data into training and testing data (using the same number of samples as in the initial file for training):\n",
    "\n",
    "x_train, y_train = x[:32561], y[:32561]\n",
    "x_test, y_test = x[32561:], y[32561:]\n",
    "\n",
    "print(f'Shape of x_train: {x_train.shape}, Shape of y_train: {y_train.shape}')\n",
    "print(f'Shape of x_test: {x_test.shape}, Shape of y_test: {y_test.shape}')\n",
    "\n",
    "# Split training data into two halves, one for training the model, the other for the attacker to use to find subpopulations\n",
    "data_split = x_train.shape[0] // 2\n",
    "x_train, y_train, x_aux, y_aux = x_train[:data_split], y_train[:data_split], x_train[data_split:], y_train[data_split:]\n",
    "\n",
    "print(f'Shape of training data: X: {x_train.shape}, Y: {y_train.shape}')\n",
    "print(f\"Shape of auxiliary data: X: {x_aux.shape}, Y: {y_aux.shape}\")\n",
    "print(f'Shape of test data: X: {x_test.shape}, Y: {y_test.shape}')\n",
    "\n",
    "# Want to create a fair split of the samples, so that we train an unbiased model\n",
    "# true_train_indices = np.where(y_train == 1)[0]\n",
    "# true_test_indices = np.where(y_test == 1)[0]\n",
    "# false_train_indices = np.where(y_train == 0)[0]\n",
    "# false_test_indices = np.where(y_test == 0)[0]\n",
    "\n",
    "# balanced_train = np.random.choice(false_train_indices.shape[0], true_train_indices.shape[0], replace=False)\n",
    "# x_train = np.concatenate((x_train[balanced_train], x_train[true_train_indices]), axis=0)\n",
    "# y_train = np.concatenate((y_train[balanced_train], y_train[true_train_indices]), axis=0)\n",
    "\n",
    "# balanced_test = np.random.choice(false_test_indices.shape[0], true_test_indices.shape[0], replace=False)\n",
    "# x_test = np.concatenate((x_test[balanced_test], x_test[true_test_indices]), axis=0)\n",
    "# y_test = np.concatenate((y_test[balanced_test], y_test[true_test_indices]), axis=0)\n",
    "\n",
    "# shuffled_train_indices = np.random.choice(x_train.shape[0], x_train.shape[0], replace=False)\n",
    "# x_train = x_train[shuffled_train_indices]\n",
    "# y_train = y_train[shuffled_train_indices]\n",
    "\n",
    "# print(f'Shape of x_train: {x_train.shape}, Shape of y_train: {y_train.shape}')\n",
    "\n",
    "# data_split = x_train.shape[0] // 2\n",
    "\n",
    "# x_train, y_train, x_aux, y_aux = x_train[:data_split], y_train[:data_split], x_train[data_split:], y_train[data_split:]\n",
    "\n",
    "# print(f'Shape of x_train: {x_train.shape}, Shape of y_train: {y_train.shape}')\n",
    "# print(f\"Shape of auxiliary data: X: {x_aux.shape}, Y: {y_aux.shape}\")\n",
    "# print(f'Shape of test data: X: {x_test.shape}, Y: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train unpoisoned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.850316319636386"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_model = linear_model.LogisticRegression(max_iter=5000)\n",
    "lin_model.fit(x_train, y_train)\n",
    "lin_model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpoisoned model accuracy: 0.8448498249493275\n"
     ]
    }
   ],
   "source": [
    "nn_model = neural_network.MLPClassifier(hidden_layer_sizes=(10,), max_iter=3000, activation='relu', random_state=42)\n",
    "nn_model.fit(x_train, y_train)\n",
    "unpoisoned_accuracy = nn_model.score(x_test, y_test)\n",
    "print(f'Unpoisoned model accuracy: {unpoisoned_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeatureMatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further data preprocessing <br>\n",
    "\"This filter function aims at matching some set of specific features of the data, that the adversary may be interested in targeting a priori\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.]]\n",
      "Shape of aux_feature: (16281, 20)\n",
      "Shape of test_feature: (16281, 20)\n",
      "Shape of train_feature: (16280, 20)\n",
      "['education_2.0', 'education_3.0', 'education_4.0', 'education_5.0', 'education_6.0', 'education_7.0', 'education_8.0', 'education_9.0', 'education_10.0', 'education_11.0', 'education_12.0', 'education_13.0', 'education_14.0', 'education_15.0', 'education_16.0', 'race_ Asian-Pac-Islander', 'race_ Black', 'race_ Other', 'race_ White', 'sex_ Male']\n",
      "There are 149 unique subpopulations in the auxiliary data\n",
      "[   8   11    2    1    2    2    3    2    1   45  140    1    2    3\n",
      "   17    1   44  222    2    1    8    4   19    1    2  239  516    2\n",
      "    1   20   28    7   39    4    6  671 1631    7   10   96   73   53\n",
      "  107    2    1  183  295    2    3   27   23    8    7    5    4  204\n",
      "  402    1    1   35   26    8   16   16   23 1136 1941   14   17  214\n",
      "  168   38   63   22   46 1265 3135   13   27  281  291   36   64    4\n",
      "   64  133    3    5   16   22    2    3    1    6  174  325    2    2\n",
      "   43   26    5    5    2    5  106  274    3    2   33   40    3    4\n",
      "    1    3   50  159    3    2   14   33    3    1   70  215    6    4\n",
      "   14   14    1    3    1   34  106    2    5    4    6    6    5    2\n",
      "    2   17   54    2    1    2    3    2    1]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Age: 0\n",
    "# Capital gain: 1\n",
    "# Capital loss: 2\n",
    "# Hours per week: 3\n",
    "# Workclass: 4 to 11 inclusive\n",
    "# Education indices: 12 to 26 inclusive\n",
    "# Marital status: 27 to 32 inclusive\n",
    "# Occupation: 33 to 46 inclusive\n",
    "# Relationship: 47 to 51 inclusive\n",
    "# Race indices: 52 to 55 inclusive\n",
    "# Sex: 56\n",
    "# Income: 57, 58 (removed from np array)\n",
    "\n",
    "# therefore len of np array is 57, 0-indexed\n",
    "\n",
    "# Paper combines race, gender and education level to create a subpopulation\n",
    "\n",
    "# 3 different poison rates used\n",
    "'''\n",
    "If the subpopulation size is m, and the adversary uses a poison-\n",
    "ing rate a relative to the subpopulation, they add am poisoned\n",
    "points, which should be small relative to the entire dataset size. In\n",
    "label flipping attacks, these points are generated by sampling am\n",
    "points satisfying the filter function from Daux and adding these\n",
    "to the training set with a label t different from the original one\n",
    "c\n",
    "'''\n",
    "poison_rates = [0.5, 1, 2]\n",
    "\n",
    "# Need to identify the unique subpopulations in the auxiliary dataset, and then \n",
    "# flip their y labels to create the poisoned dataset\n",
    "# Then train new models on these datasets, and test the clean test datasets on both\n",
    "# the clean and poisoned models\n",
    "\n",
    "aux_feature = np.concatenate((x_aux[:, 12:27], x_aux[:, 52:57]), axis=1)\n",
    "test_feature = np.concatenate((x_test[:, 12:27], x_test[:, 52:57]), axis=1)\n",
    "train_feature = np.concatenate((x_train[:, 12:27], x_train[:, 52:57]), axis=1)\n",
    "\n",
    "print(aux_feature[0:5])\n",
    "\n",
    "print(f'Shape of aux_feature: {aux_feature.shape}')\n",
    "print(f'Shape of test_feature: {test_feature.shape}')\n",
    "print(f'Shape of train_feature: {train_feature.shape}')\n",
    "\n",
    "column_names = list(uci_data.columns)\n",
    "feature_columns = column_names[12:27] + column_names[52:57]\n",
    "print(feature_columns)\n",
    "\n",
    "subpopulations, subpopulation_counts = np.unique(aux_feature, axis=0, return_counts=True)\n",
    "\n",
    "print(f'There are {len(subpopulations)} unique subpopulations in the auxiliary data')\n",
    "print(subpopulation_counts)\n",
    "\n",
    "# print(f'There are {len(subpopulations)} unique subpopulations in the auxiliary data')\n",
    "\n",
    "# subpop_confidence, valid_subpopulations = [], []\n",
    "\n",
    "# linear_regression_errors, neural_net_errors = np.zeros((len(subpopulations), 3, len(poison_rates))), np.zeros((len(subpopulations), 3, len(poison_rates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 71 valid subpopulations in the auxiliary data\n"
     ]
    }
   ],
   "source": [
    "valid_subpopulations = [(subpop, count) for subpop, count in zip(subpopulations, subpopulation_counts) if 10 < count]\n",
    "print(f'There are {len(valid_subpopulations)} valid subpopulations in the auxiliary data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subpopulation 0, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8356734269370775\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8316432657306292\n",
      "Subpopulation 0, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8361934477379095\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8242329693187728\n",
      "Subpopulation 0, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8376235049401975\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8278731149245969\n",
      "Subpopulation 1, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8363234529381175\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8283931357254289\n",
      "Subpopulation 1, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8382735309412377\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8203328133125325\n",
      "Subpopulation 1, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8359334373374935\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8312532501300053\n",
      "Subpopulation 2, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8369734789391575\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8244929797191888\n",
      "Subpopulation 2, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8358034321372855\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8278731149245969\n",
      "Subpopulation 2, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8345033801352054\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8072022880915236\n",
      "Subpopulation 3, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8350234009360374\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8334633385335414\n",
      "Subpopulation 3, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8332033281331254\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8137025481019241\n",
      "Subpopulation 3, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8273530941237649\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8241029641185648\n",
      "Subpopulation 4, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8365834633385335\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8326833073322933\n",
      "Subpopulation 4, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8369734789391575\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8321632865314612\n",
      "Subpopulation 4, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8364534581383255\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8303432137285491\n",
      "Subpopulation 5, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8360634425377015\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8212428497139884\n",
      "Subpopulation 5, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8352834113364535\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8302132085283412\n",
      "Subpopulation 5, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8364534581383255\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8302132085283412\n",
      "Subpopulation 6, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8343733749349974\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8332033281331254\n",
      "Subpopulation 6, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8347633905356214\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.81071242849714\n",
      "Subpopulation 6, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8313832553302132\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8247529901196048\n",
      "Subpopulation 7, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8360634425377015\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8321632865314612\n",
      "Subpopulation 7, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8358034321372855\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8312532501300053\n",
      "Subpopulation 7, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8360634425377015\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.830473218928757\n",
      "Subpopulation 8, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8368434737389495\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8189027561102445\n",
      "Subpopulation 8, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8361934477379095\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8252730109204368\n",
      "Subpopulation 8, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8342433697347894\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8247529901196048\n",
      "Subpopulation 9, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8358034321372855\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8348933957358294\n",
      "Subpopulation 9, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8356734269370775\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8367134685387415\n",
      "Subpopulation 9, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8367134685387415\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8322932917316693\n",
      "Subpopulation 10, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8359334373374935\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8343733749349974\n",
      "Subpopulation 10, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8354134165366615\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8333333333333334\n",
      "Subpopulation 10, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8350234009360374\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8244929797191888\n",
      "Subpopulation 11, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8368434737389495\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8354134165366615\n",
      "Subpopulation 11, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8367134685387415\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8248829953198128\n",
      "Subpopulation 11, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8360634425377015\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8274830993239729\n",
      "Subpopulation 12, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8363234529381175\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8345033801352054\n",
      "Subpopulation 12, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8364534581383255\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8337233489339574\n",
      "Subpopulation 12, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8372334893395735\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8306032241289651\n",
      "Subpopulation 13, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8363234529381175\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.830473218928757\n",
      "Subpopulation 13, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8391835673426936\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8352834113364535\n",
      "Subpopulation 13, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8359334373374935\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8257930317212688\n",
      "Subpopulation 14, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8356734269370775\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8248829953198128\n",
      "Subpopulation 14, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8380135205408217\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8298231929277171\n",
      "Subpopulation 14, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8372334893395735\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8308632345293812\n",
      "Subpopulation 15, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8360634425377015\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8246229849193968\n",
      "Subpopulation 15, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8367134685387415\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8242329693187728\n",
      "Subpopulation 15, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8352834113364535\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8248829953198128\n",
      "Subpopulation 16, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8363234529381175\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8346333853354134\n",
      "Subpopulation 16, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8367134685387415\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8159126365054602\n",
      "Subpopulation 16, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8374934997399895\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8325533021320853\n",
      "Subpopulation 17, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8371034841393655\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8306032241289651\n",
      "Subpopulation 17, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8363234529381175\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8319032761310452\n",
      "Subpopulation 17, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8346333853354134\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8261830473218928\n",
      "Subpopulation 18, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8364534581383255\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8351534061362454\n",
      "Subpopulation 18, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8361934477379095\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.830993239729589\n",
      "Subpopulation 18, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8369734789391575\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8319032761310452\n",
      "Subpopulation 19, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8373634945397815\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.828263130525221\n",
      "Subpopulation 19, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8342433697347894\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8298231929277171\n",
      "Subpopulation 19, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8342433697347894\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8204628185127406\n",
      "Subpopulation 20, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8372334893395735\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8350234009360374\n",
      "Subpopulation 20, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8363234529381175\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8308632345293812\n",
      "Subpopulation 20, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8378835153406138\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8291731669266772\n",
      "Subpopulation 21, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8363234529381175\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8338533541341654\n",
      "Subpopulation 21, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8372334893395735\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8316432657306292\n",
      "Subpopulation 21, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8367134685387415\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8337233489339574\n",
      "Subpopulation 22, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8372334893395735\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8317732709308373\n",
      "Subpopulation 22, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8351534061362454\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8319032761310452\n",
      "Subpopulation 22, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8356734269370775\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8290431617264691\n",
      "Subpopulation 23, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8354134165366615\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8281331253250132\n",
      "Subpopulation 23, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8346333853354134\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8303432137285491\n",
      "Subpopulation 23, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8329433177327094\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8239729589183568\n",
      "Subpopulation 24, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8348933957358294\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8317732709308373\n",
      "Subpopulation 24, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8339833593343734\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8337233489339574\n",
      "Subpopulation 24, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8324232969318772\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.828003120124805\n",
      "Subpopulation 25, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8372334893395735\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8339833593343734\n",
      "Subpopulation 25, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8347633905356214\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8334633385335414\n",
      "Subpopulation 25, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8352834113364535\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8307332293291733\n",
      "Subpopulation 26, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8348933957358294\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8257930317212688\n",
      "Subpopulation 26, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8354134165366615\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8312532501300053\n",
      "Subpopulation 26, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8333333333333334\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8152626105044202\n",
      "Subpopulation 27, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8350234009360374\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8317732709308373\n",
      "Subpopulation 27, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8341133645345814\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8307332293291733\n",
      "Subpopulation 27, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8352834113364535\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8326833073322933\n",
      "Subpopulation 28, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8363234529381175\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8363234529381175\n",
      "Subpopulation 28, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8363234529381175\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8332033281331254\n",
      "Subpopulation 28, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8345033801352054\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8283931357254289\n",
      "Subpopulation 29, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8374934997399895\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8315132605304211\n",
      "Subpopulation 29, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8360634425377015\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8339833593343734\n",
      "Subpopulation 29, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8347633905356214\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8269630785231409\n",
      "Subpopulation 30, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8354134165366615\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8345033801352054\n",
      "Subpopulation 30, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8378835153406138\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8313832553302132\n",
      "Subpopulation 30, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8363234529381175\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8356734269370775\n",
      "Subpopulation 31, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8358034321372855\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8302132085283412\n",
      "Subpopulation 31, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8350234009360374\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8315132605304211\n",
      "Subpopulation 31, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8335933437337494\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8312532501300053\n",
      "Subpopulation 32, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8372334893395735\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8320332813312531\n",
      "Subpopulation 32, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8373634945397815\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8339833593343734\n",
      "Subpopulation 32, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8369734789391575\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.827743109724389\n",
      "Subpopulation 33, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8373634945397815\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8265730629225169\n",
      "Subpopulation 33, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8360634425377015\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8273530941237649\n",
      "Subpopulation 33, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8363234529381175\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8337233489339574\n",
      "Subpopulation 34, Poison rate 0.5\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8365834633385335\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8308632345293812\n",
      "Subpopulation 34, Poison rate 1\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8364534581383255\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8321632865314612\n",
      "Subpopulation 34, Poison rate 2\n",
      "Clean Linear regression test accuracy: 0.8372334893395735\n",
      "Poisoned Linear regression test accuracy: 0.8368434737389495\n",
      "Clean Neural network test accuracy: 0.8348933957358294\n",
      "Poisoned Neural network test accuracy: 0.8315132605304211\n"
     ]
    }
   ],
   "source": [
    "# for i, (subpop, count) in enumerate(valid_subpopulations):\n",
    "#     test_subpopulation = np.where(np.linalg.norm(test_feature - subpop, axis=1) == 0)\n",
    "#     aux_subpopulation = np.where(np.linalg.norm(aux_feature - subpop, axis=1) == 0)\n",
    "#     train_subpopulation = np.where(np.linalg.norm(train_feature - subpop, axis=1) == 0)\n",
    "\n",
    "#     aux_test_x, aux_test_y = x_aux[test_subpopulation], y_aux[test_subpopulation]\n",
    "#     poison_x_base, poison_y_base = x_test[test_subpopulation], y_test[test_subpopulation]\n",
    "\n",
    "#     train_count = train_subpopulation[0].shape[0]\n",
    "#     test_count = aux_test_x.shape[0]\n",
    "\n",
    "#     for j, poison_count in enumerate([int(train_count * rate) for rate in poison_rates]):\n",
    "#         poison_indices = np.random.choice(poison_x_base.shape[0], poison_count, replace=True)\n",
    "#         poison_x, poison_y = poison_x_base[poison_indices], 1 - poison_y_base[poison_indices]\n",
    "\n",
    "#         total_x, total_y = np.concatenate((x_train, poison_x), axis=0), np.concatenate((y_train, poison_y), axis=0)\n",
    "\n",
    "#         lr_models = [linear_model.LogisticRegression(max_iter=5000) for _ in range(3)]\n",
    "#         for model in lr_models:\n",
    "#             model.fit(total_x, total_y)\n",
    "\n",
    "#         nn_models = [neural_network.MLPClassifier(hidden_layer_sizes=(10,), max_iter=3000, activation='relu', random_state=42) for _ in range(3)]\n",
    "#         for model in nn_models:\n",
    "#             model.fit(total_x, total_y)\n",
    "\n",
    "#         clean_lin_test_acc = lin_model.score(x_test, y_test)\n",
    "#         poisoned_lin_test_acc = np.mean([model.score(x_test, y_test) for model in lr_models])\n",
    "#         clean_nn_test_acc = nn_model.score(x_test, y_test)\n",
    "#         poisoned_nn_test_acc = np.mean([model.score(x_test, y_test) for model in nn_models])\n",
    "\n",
    "#         print(f'Subpopulation {i}, Poison rate {poison_rates[j]}')\n",
    "#         print(f'Clean Linear regression test accuracy: {clean_lin_test_acc}')\n",
    "#         print(f'Poisoned Linear regression test accuracy: {poisoned_lin_test_acc}')\n",
    "#         print(f'Clean Neural network test accuracy: {clean_nn_test_acc}')\n",
    "#         print(f'Poisoned Neural network test accuracy: {poisoned_nn_test_acc}')\n",
    "#         print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (subpop, count) in enumerate(zip(subpopulations, subpopulation_counts)):\n",
    "#     if count > 10 and count < 100:\n",
    "#         valid_subpopulations.append((i, count))\n",
    "#         print(\"\\n\\n\")\n",
    "#         print(f'Subpopulation {i} has count {count} occurences')\n",
    "\n",
    "#         test_subpopulation = np.where(np.linalg.norm(test_feature - subpop, axis=1) == 0)\n",
    "#         aux_subpopulation = np.where(np.linalg.norm(aux_feature - subpop, axis=1) == 0)\n",
    "#         train_subpopulation = np.where(np.linalg.norm(train_feature - subpop, axis=1) == 0)\n",
    "\n",
    "#         aux_test_x, aux_test_y = x_aux[test_subpopulation], y_aux[test_subpopulation]\n",
    "\n",
    "#         poison_x_base, poison_y_base = x_test[test_subpopulation], y_test[test_subpopulation]\n",
    "\n",
    "#         train_count = train_subpopulation[0].shape[0]\n",
    "#         test_count = aux_test_x.shape[0]\n",
    "\n",
    "#         #subpop_confidence.append(2*np.multiply(lin_model.predict_proba(aux_test_x), np.eye(2)[aux_test_y.astype(int)]).mean())\n",
    "\n",
    "#         for j, poison_count in enumerate([int(train_count * rate) for rate in poison_rates]):\n",
    "#             poison_indices = np.random.choice(poison_x_base.shape[0], poison_count, replace=True)\n",
    "\n",
    "#             poison_x, poison_y = poison_x_base[poison_indices], 1 - poison_y_base[poison_indices]\n",
    "\n",
    "#             total_x, total_y = np.concatenate((x_train, poison_x), axis=0), np.concatenate((y_train, poison_y), axis=0)\n",
    "\n",
    "#             print(f'Subpopulation {i}, Poison rate {poison_rates[j]}')\n",
    "#             print([feature for x, feature in zip(subpop, feature_columns) if x > 0.5])\n",
    "\n",
    "#             lr_models = [linear_model.LogisticRegression(max_iter=5000) for _ in range(3)]\n",
    "#             for model in lr_models:\n",
    "#                 model.fit(total_x, total_y)\n",
    "\n",
    "#             nn_models = [neural_network.MLPClassifier(hidden_layer_sizes=(10,), max_iter=3000, activation='relu', random_state=42) for _ in range(3)]\n",
    "#             for model in nn_models:\n",
    "#                 model.fit(total_x, total_y)\n",
    "\n",
    "#             clean_lin_test_acc = lin_model.score(x_test, y_test)\n",
    "#             print(f'Clean Linear regression test accuracy: {clean_lin_test_acc}')\n",
    "\n",
    "#             poisoned_lin_test_acc = np.mean([model.score(x_test, y_test) for model in lr_models])\n",
    "#             print(f'Poisoned Linear regression test accuracy: {poisoned_lin_test_acc}')\n",
    "\n",
    "#             clean_nn_test_acc = nn_model.score(x_test, y_test)\n",
    "#             print(f'Clean Neural network test accuracy: {clean_nn_test_acc}')\n",
    "\n",
    "#             poisoned_nn_test_acc = np.mean([model.score(x_test, y_test) for model in nn_models])\n",
    "#             print(f'Poisoned Neural network test accuracy: {poisoned_nn_test_acc}')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
