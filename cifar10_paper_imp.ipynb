{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2)\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2)\n",
    "        )\n",
    "        self.final_conv = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.mean_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x, extract_layer=None, mode=\"features\"):\n",
    "        x = self.conv1(x)\n",
    "        if extract_layer == 1: return x\n",
    "        x = self.block1(x)\n",
    "        if extract_layer == 2: return x\n",
    "        x = self.block2(x)\n",
    "        if extract_layer == 3: return x\n",
    "        x = self.block3(x)\n",
    "        if extract_layer == 4: return x\n",
    "        x = self.final_conv(x)\n",
    "        if extract_layer == 5: return x\n",
    "        x = self.mean_pool(x).view(x.size(0), -1)\n",
    "        if extract_layer == 6: return x\n",
    "\n",
    "        if mode == \"classify\":\n",
    "            return self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([3, 32, 32])\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "cifar_mean = [0.4914, 0.4822, 0.4465]\n",
    "cifar_std = [0.2470, 0.2435, 0.2616]\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(cifar_mean, cifar_std)])\n",
    "\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "d_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "d_test = torch.utils.data.Subset(d_test, range(len(d_test)))\n",
    "\n",
    "d_train, d_aux = torch.utils.data.random_split(dataset, [25000, 25000])\n",
    "\n",
    "img, label = d_train[0]\n",
    "print(img.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.array([label for _, label in d_test])\n",
    "train_labels = np.array([label for _, label in d_train])\n",
    "aux_labels = np.array([label for _, label in d_aux])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "<class 'tuple'>\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(d_train[0][1]) # label\n",
    "\n",
    "sample = d_train[0]\n",
    "print(sample[1])\n",
    "print(type(sample))\n",
    "print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_classification(model, dataloader, epochs=10):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "    optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            outputs = model(images, mode=\"classify\")\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        print(f\"Epoch {epoch}: Loss: {total_loss / len(dataloader)}, Accuracy: {100 * correct/total:.2f}%\")\n",
    "        \n",
    "\n",
    "def train_cnn(model, dataloader, epochs=10):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "    optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimiser.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, dataloader, layer):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in dataloader:\n",
    "            feature = model(images, extract_layer=layer)\n",
    "            features.append(feature.view(feature.size(0), -1).cpu().numpy())\n",
    "    return np.vstack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images, mode=\"classify\")\n",
    "            _, predicted = outputs.max(1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "    return predictions\n",
    "\n",
    "def evaluate_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images, mode=\"classify\")\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch 0: Loss: 1.8490378975563342, Accuracy: 27.95%\n",
      "Epoch 1: Loss: 1.4234001430709038, Accuracy: 45.92%\n",
      "Epoch 2: Loss: 1.1606690760158822, Accuracy: 57.28%\n",
      "Epoch 3: Loss: 0.9839992132943, Accuracy: 64.80%\n",
      "Epoch 4: Loss: 0.8476954913505202, Accuracy: 69.78%\n",
      "Epoch 5: Loss: 0.7308976592310249, Accuracy: 74.02%\n",
      "Epoch 6: Loss: 0.6236804678769368, Accuracy: 78.00%\n",
      "Epoch 7: Loss: 0.5320590131575494, Accuracy: 81.14%\n",
      "Epoch 8: Loss: 0.44548755583098476, Accuracy: 84.27%\n",
      "Epoch 9: Loss: 0.36456434630676915, Accuracy: 87.02%\n"
     ]
    }
   ],
   "source": [
    "dataloader_aux = torch.utils.data.DataLoader(d_aux, batch_size=64, shuffle=True)\n",
    "cnn_model = ConvNet()\n",
    "train_for_classification(cnn_model, dataloader_aux, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_acc = evaluate_accuracy(cnn_model, dataloader_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = torch.utils.data.DataLoader(d_train, batch_size=64, shuffle=False)\n",
    "train_features = {layer: extract_features(cnn_model, dataloader_train, layer) for layer in range(1, 7)}\n",
    "\n",
    "aux_features = {layer: extract_features(cnn_model, dataloader_aux, layer) for layer in range(1, 7)}\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(d_test, batch_size=64, shuffle=False)\n",
    "test_features = {layer: extract_features(cnn_model, dataloader_test, layer) for layer in range(1, 7)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(dataloader_aux.dataset[0][0].shape)\n",
    "print(dataloader_test.dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_pca_models = {layer: PCA(n_components=10).fit(aux_features[layer]) for layer in aux_features}\n",
    "aux_pca_features = {layer: aux_pca_models[layer].transform(aux_features[layer]) for layer in aux_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca_models = {layer: PCA(n_components=10).fit(train_features[layer]) for layer in train_features}\n",
    "train_pca_features = {layer: train_pca_models[layer].transform(train_features[layer]) for layer in train_features}\n",
    "\n",
    "test_pca_models = {layer: PCA(n_components=10).fit(test_features[layer]) for layer in test_features}\n",
    "test_pca_features = {layer: test_pca_models[layer].transform(test_features[layer]) for layer in test_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "kmeans_models = {layer: KMeans(n_clusters=100, random_state=42).fit(aux_pca_features[layer]) for layer in aux_features}\n",
    "cluster_labels = {layer: kmeans_models[layer].labels_ for layer in aux_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.cluster._kmeans.KMeans'>\n"
     ]
    }
   ],
   "source": [
    "print(type(kmeans_models[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 unique clusters in the auxiliary data\n",
      "Cluster counts: [256 419 481 345 225 129 388 249 422 266 277 135 298 292  48  69 157 268\n",
      " 252 245 360 198 293 398 239 523 382 175 332 223 315 142 174 109 243 318\n",
      "  99 211 120 510 289  67 287 331 274  76 169 240 229 290 243 272 409 268\n",
      " 263  71 316 615  66 135 204  93 277 220 221 247 342 185 256  94 219 188\n",
      " 283 168 142 223 322 213 389 202 384 292 381 132 183 254 113 143 237 221\n",
      " 190 324 190 411 214 300 227 222 339 260]\n",
      "Cluster indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n"
     ]
    }
   ],
   "source": [
    "layer_1 = kmeans_models[1]\n",
    "\n",
    "cluster_indices, cluster_counts = np.unique(layer_1.labels_, return_counts=True)\n",
    "\n",
    "print(f'There are {len(cluster_indices)} unique clusters in the auxiliary data')\n",
    "print(f'Cluster counts: {cluster_counts}')\n",
    "print(f'Cluster indices: {cluster_indices}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0:\n",
      "There are 100 unique clusters in the auxiliary data\n",
      "Cluster counts: [256 419 481 345 225 129 388 249 422 266 277 135 298 292  48  69 157 268\n",
      " 252 245 360 198 293 398 239 523 382 175 332 223 315 142 174 109 243 318\n",
      "  99 211 120 510 289  67 287 331 274  76 169 240 229 290 243 272 409 268\n",
      " 263  71 316 615  66 135 204  93 277 220 221 247 342 185 256  94 219 188\n",
      " 283 168 142 223 322 213 389 202 384 292 381 132 183 254 113 143 237 221\n",
      " 190 324 190 411 214 300 227 222 339 260]\n",
      "Cluster indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "Model 1:\n",
      "There are 100 unique clusters in the auxiliary data\n",
      "Cluster counts: [ 186  335  122  176  751  343  267   42   48  173  159   50  493  371\n",
      "  261  230  115  843  719  266  100  261  659  256  104  414  251  118\n",
      "   93  372  138  203  164  258  129  135  220  160  416  146  186   29\n",
      "  350  125   98  535  294  114  221   54   96  764   76   65  190  357\n",
      "  292  344  105  163  344   59  277  107  181   59  236  170  200  295\n",
      "  146  163  409  166  268  122  127  599 1064  235  473  142  399   23\n",
      "  304  184  621  123  160  332  203  232   98  259  143  238   81  162\n",
      "  234  357]\n",
      "Cluster indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "Model 2:\n",
      "There are 100 unique clusters in the auxiliary data\n",
      "Cluster counts: [ 569  248  225  212  344  213  226  232  631  128  268  449  129  172\n",
      "  193  312 1035  163   48  220  225  123  327  169   79  314  155  344\n",
      "  139  488  155   67  104  139  270  534  415   61  209  989  269  230\n",
      "  394  216  124  126  515  209  125  130  113  402  228  180  242  124\n",
      "  179  165  299   73  146  107   48   71  219  218  242  269  124  282\n",
      "  335  264  135  228   61  155  176  605   76  157  205  360  353  156\n",
      "  200  956   94  150  471  150  489  110  150  101  136  292  445  158\n",
      "  246  194]\n",
      "Cluster indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "Model 3:\n",
      "There are 100 unique clusters in the auxiliary data\n",
      "Cluster counts: [ 259  149  332  158  117  749  131  432  842   74  177   34  942  138\n",
      "   29   38  563  173   93  153   75  128  244  896   57  349   56  118\n",
      "  169  453   74  236   74  251  206  146  115  648  915  312  102  658\n",
      "  354  166   69  298  235  223  146  187  289   30   96   48  446   41\n",
      "   98  137  435  298   58 1001  111  137   56  620  193   52  210   73\n",
      "   50  413  122   83  140   65 1007   62  216  296  483  148  353  381\n",
      "  105   58  349  259   62  131   15  131  150  125   38  314  577  477\n",
      "  235  183]\n",
      "Cluster indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "Model 4:\n",
      "There are 100 unique clusters in the auxiliary data\n",
      "Cluster counts: [301 149 424 499 122 130 154 139 540 508 129 118 281 251 288 182 374 489\n",
      " 319  42 174  66 243 158 330 538  46 268 326 450 146 304 144 486 157 148\n",
      " 121 182 231  52 108 127 131 243 361 205 508  92 326 114 506 163 306 144\n",
      " 336 120 569 285 341  53 558 500 178 815 296  80 158  75 257 206  46 123\n",
      " 121 190 184 178 192 248 539 164 187 138 284 161 101 145 406 233 346 473\n",
      " 123 577 134 247 160  95 105 258 150 522]\n",
      "Cluster indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "Model 5:\n",
      "There are 100 unique clusters in the auxiliary data\n",
      "Cluster counts: [224 350 156 183 246 542 145 225 261 273 161 433 349 229 291  76 433 195\n",
      " 139 323 233 257 104 317 444 399 112 145 409 200 205 230 348 320 435  60\n",
      " 215 363 221 153 206 191 403 380 222  94 175 159  97 197 132 406 243 227\n",
      " 386 180 453  45 120 195  66 581 285 411 201 302 179 341 143 366 205 154\n",
      " 368 139 234 431 123 109 226 174 134 294 217 532 126 216 402 373 210 319\n",
      " 112 509  69 151  49 220 392 195 247 280]\n",
      "Cluster indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n"
     ]
    }
   ],
   "source": [
    "clusters = []\n",
    "km_data = []\n",
    "\n",
    "for layer, model in kmeans_models.items():\n",
    "    test_km, train_km = model.predict(test_pca_features[layer]), model.predict(train_pca_features[layer])\n",
    "    km_data.append((test_km, train_km))\n",
    "    indices, counts = np.unique(model.labels_, return_counts=True)\n",
    "    clusters.append((indices, counts))\n",
    "\n",
    "for i, (indices, counts) in enumerate(clusters):\n",
    "    print(f'Model {i}:')\n",
    "    print(f'There are {len(indices)} unique clusters in the auxiliary data')\n",
    "    print(f'Cluster counts: {counts}')\n",
    "    print(f'Cluster indices: {indices}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x13a3ed310>\n"
     ]
    }
   ],
   "source": [
    "print(dataloader_aux.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x13a3ed310>\n",
      "<torch.utils.data.dataset.Subset object at 0x13da694d0>\n"
     ]
    }
   ],
   "source": [
    "print(dataloader_aux.dataset)\n",
    "print(dataloader_test.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model 0:\n",
      "\n",
      "\n",
      "Cluster index: 0, Cluster Count: 256, Test Samples: 68\n",
      "Poison rate: 0.5\n",
      "Number of poisoned samples: 140\n",
      "Original dataset size: 25000\n",
      "Poisoned dataset size: 25140\n",
      "cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m poison_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(d_train_poisoned, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     60\u001b[0m poisoned_model \u001b[38;5;241m=\u001b[39m ConvNet()\n\u001b[0;32m---> 61\u001b[0m \u001b[43mtrain_for_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoisoned_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoison_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m clean_score \u001b[38;5;241m=\u001b[39m baseline_acc\n\u001b[1;32m     64\u001b[0m poisoned_model_clean_subpop_score \u001b[38;5;241m=\u001b[39m evaluate_accuracy(poisoned_model, subpop_test_dataloader) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(test_samples) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mtrain_for_classification\u001b[0;34m(model, dataloader, epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassify\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "poison_rates = [0.5, 1, 2]\n",
    "\n",
    "# k_valid_subpopulations = [(subpop, count) for subpop, count in zip(cluster_indices, cluster_counts)]\n",
    "# k_nn_data = np.zeros((len(k_valid_subpopulations), 6, len(poison_rates)))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    valid_subpopulations = [(subpop, count) for subpop, count in zip(clusters[i][0], clusters[i][1])]\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(f'Model {i}:')\n",
    "\n",
    "    for j, (index, count) in enumerate(valid_subpopulations):\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(f\"Cluster index: {j}, Cluster Count: {count}, Test Samples: {np.where(km_data[i][0] == index)[0].shape[0]}\")\n",
    "\n",
    "        test_indices = np.where(test_km == index)[0]\n",
    "        train_indices = np.where(train_km == index)\n",
    "        aux_indices = np.where(kmeans_models[i+1].labels_ == index)[0]\n",
    "\n",
    "        test_samples = [dataloader_test.dataset[x][0] for x in test_indices]\n",
    "        test_samples_labels = [dataloader_test.dataset[x][1] for x in test_indices]\n",
    "\n",
    "        test_dataset = list(zip(test_samples, test_samples_labels))\n",
    "        test_subset = torch.utils.data.Subset(test_dataset, range(len(test_samples)))\n",
    "        subpop_test_dataloader = torch.utils.data.DataLoader(test_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "        aux_samples = [dataloader_aux.dataset[x][0] for x in aux_indices]\n",
    "        aux_samples_labels = [dataloader_aux.dataset[x][1] for x in aux_indices]\n",
    "\n",
    "        aux_dataset = list(zip(aux_samples, aux_samples_labels))\n",
    "        aux_subset = torch.utils.data.Subset(aux_dataset, range(len(aux_samples)))\n",
    "        subpop_aux_dataloader = torch.utils.data.DataLoader(aux_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "        train_count = train_indices[0].shape[0]\n",
    "        \n",
    "        for k, poison_count in enumerate([int(train_count * rate) for rate in poison_rates]):\n",
    "\n",
    "            print(f'Poison rate: {poison_rates[k]}')\n",
    "            print(f'Number of poisoned samples: {poison_count}')\n",
    "\n",
    "            #poison_indices = np.random.choice(aux_samples.shape[0], poison_count, replace=True)\n",
    "            poison_indices = np.random.choice(range(len(aux_samples)), poison_count, replace=True)\n",
    "\n",
    "            poison_samples = [aux_samples[x] for x in poison_indices]\n",
    "            poison_samples_labels = [aux_samples_labels[x] for x in poison_indices]\n",
    "\n",
    "            poison_dataset = list(zip(poison_samples, poison_samples_labels))\n",
    "            poison_subset = torch.utils.data.Subset(poison_dataset, range(len(poison_samples)))\n",
    "\n",
    "            d_train_poisoned = torch.utils.data.ConcatDataset([d_train, poison_subset]) if poison_samples else d_train\n",
    "\n",
    "            print(f'Original dataset size: {len(d_train)}')\n",
    "            print(f'Poisoned dataset size: {len(d_train_poisoned)}')\n",
    "\n",
    "            poison_dataloader = torch.utils.data.DataLoader(d_train_poisoned, batch_size=64, shuffle=True)\n",
    "\n",
    "            poisoned_model = ConvNet()\n",
    "            train_for_classification(poisoned_model, poison_dataloader, epochs=10)\n",
    "\n",
    "            clean_score = baseline_acc\n",
    "            poisoned_model_clean_subpop_score = evaluate_accuracy(poisoned_model, subpop_test_dataloader) if len(test_samples) > 0 else 0\n",
    "            clean_model_clean_subpop_score = evaluate_accuracy(cnn_model, subpop_test_dataloader) if len(test_samples) > 0 else 0\n",
    "            poisoned_model_clean_test_data_score = evaluate_accuracy(poisoned_model, dataloader_test)\n",
    "            clean_model_poison_data_score = evaluate_accuracy(cnn_model, subpop_aux_dataloader)\n",
    "            poisoned_model_poison_data_score = evaluate_accuracy(poisoned_model, subpop_aux_dataloader)\n",
    "\n",
    "            print(f'Clean Model Accuracy: {clean_score}')\n",
    "            print(f'Poisoned Model, Clean Subpopulation accuracy (target): {poisoned_model_clean_subpop_score}')\n",
    "            print(f'Clean Model, Clean Subpopulation accuracy: {clean_model_clean_subpop_score}')\n",
    "            print(f'Number of samples tested on poisoned model: {len(test_samples)}')\n",
    "            print(f'Poisoned Model, Clean Test Data accuracy (collateral): {poisoned_model_clean_test_data_score}')\n",
    "            print(f'Clean Model, Poison Data accuracy: {clean_model_poison_data_score}')\n",
    "            print(f'Poisoned Model, Poison Data accuracy: {poisoned_model_poison_data_score}')\n",
    "\n",
    "            print(\"\\n\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m poison_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(d_train_poisoned, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m poison_model \u001b[38;5;241m=\u001b[39m ConvNet()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_for_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoison_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoison_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mtrain_for_classification\u001b[0;34m(model, dataloader, epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassify\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "poison_dataloader = torch.utils.data.DataLoader(d_train_poisoned, batch_size=64, shuffle=True)\n",
    "\n",
    "poison_model = ConvNet()\n",
    "train_for_classification(poison_model, poison_dataloader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'int'>\n",
      "8\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'int'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# print(d_train)\n",
    "# print(d_train[-1])\n",
    "print(type(d_train[-1]))\n",
    "print(type(d_train[-1][0]))\n",
    "print(type(d_train[-1][-1]))\n",
    "print(d_train[-1][-1])\n",
    "x_train_poisoned = torch.utils.data.ConcatDataset([d_train, poison_subset]) if poison_samples else d_train\n",
    "# print(x_train_poisoned)\n",
    "# print(x_train_poisoned[-1])\n",
    "print(type(x_train_poisoned[-1]))\n",
    "print(type(x_train_poisoned[-1][0]))\n",
    "print(type(x_train_poisoned[-1][-1]))\n",
    "print(x_train_poisoned[-1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
